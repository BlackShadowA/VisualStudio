import findspark
findspark.init()
import pandas as pd
import numpy as np
from pyspark.sql import functions as F
#import cloudpickle as pickle

#Spark Ui http://localhost:4040
from pyspark.sql.session import SparkSession
import pyspark
import os
os.environ["HADOOP_HOME"]= 'C:\\Travaux_2012\\Anaconda e Python\\hadoop-2.8.1'

spark = SparkSession\
    .builder\
    .appName("PySpark XGBOOST Native")\
    .getOrCreate()

print(f"Versione Pyspark = {spark.version}")

my_dict = {
    'KEY':[11,22,33],
    'VALORI':['CC','ll','BB']
}

ll = spark.createDataFrame(pd.DataFrame(my_dict))
ll.show()

from functools import reduce

actual_df = (reduce(lambda memo_df, col_name: memo_df.withColumn(col_name, F.lower(F.col(col_name))), ll.columns, ll))
actual_df.show()

# SOMMO LE COLONNE NUMERICHE

actual_df = actual_df.select(reduce("KEY", F.lit(0.0), lambda acc, x: acc + x).alias("sum")).show()